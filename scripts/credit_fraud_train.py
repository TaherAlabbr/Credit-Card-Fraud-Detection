import pickle
import time
from pathlib import Path

from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier, VotingClassifier
from sklearn.neural_network import MLPClassifier
from imblearn.pipeline import Pipeline
from sklearn.model_selection import StratifiedKFold
from sklearn.neighbors import KNeighborsClassifier

from cli_args import parse_args
from credit_fraud_utils_data import *
from credit_fraud_utils_eval import evaluate_with_threshold, find_best_threshold


def build_model(args, scaler, balance_strategy):
    """
    Build a sklearn pipeline with optional balancing, scaling, PCA, and a classifier.

    - Use class_weight if cost-sensitive learning enabled.
    - Voting classifier uses weighted soft voting.
    """
    class_weight = {0: 1, 1: 3} if args.cost_sensitive else None

    # Initialize the base classifier
    if args.model == 'LR':
        model = LogisticRegression(C=args.lr_c, max_iter=args.lr_max_iter,
                                   class_weight=class_weight, random_state=42)
    elif args.model == 'RF':
        model = RandomForestClassifier(n_estimators=args.rf_n_estimators,
                                       max_depth=args.rf_max_depth,
                                       class_weight=class_weight,
                                       random_state=42)
    elif args.model == 'NN':
        model = MLPClassifier(hidden_layer_sizes=args.nn_hidden_layers,
                              activation=args.nn_activation,
                              max_iter=args.nn_max_iter,
                              alpha=args.nn_alpha,
                              random_state=42)
    elif args.model == 'KNN':
        model = KNeighborsClassifier(n_neighbors=args.knn_n_neighbors,
                                     weights=args.knn_weights)
    elif args.model == 'VC':
        clf1 = RandomForestClassifier(n_estimators=args.rf_n_estimators,
                                      max_depth=args.rf_max_depth,
                                      class_weight=class_weight,
                                      random_state=42)
        clf2 = MLPClassifier(hidden_layer_sizes=args.nn_hidden_layers,
                             activation=args.nn_activation,
                             max_iter=args.nn_max_iter,
                             alpha=args.nn_alpha,
                             random_state=42)
        clf3 = KNeighborsClassifier(n_neighbors=args.knn_n_neighbors,
                                    weights=args.knn_weights)
        model = VotingClassifier(
            estimators=[('rf', clf1), ('nn', clf2), ('knn', clf3)],
            voting='soft',
            weights=[5, 3, 4]
        )
    else:
        raise ValueError(f"Unknown model '{args.model}'")

    steps = [] # Order is important: resample --> scale --> train
    if args.balance == 'hybrid': # note that the imblearn pipeline does not allow to add intermediate pipeline!
        for strategy in balance_strategy: # Add each (name, transformer) tuple
            steps.append((strategy[0], strategy[1]))

    elif balance_strategy and args.balance != 'cost_sensitive':
        steps.append(('balance', balance_strategy))

    if scaler:
        steps.append(('scaler', scaler))

    if args.use_pca:        
        from sklearn.decomposition import PCA
        pca = PCA(n_components=args.n_components, random_state=42)
        steps.append(('pca', pca)) 
        # → Resample → Scale → PCA → Train
        # Never apply PCA before scaling. Never apply PCA before resampling.

    steps.append(('model', model))
    pipeline = Pipeline(steps)

    return pipeline


def save_model_bundle(pipeline, threshold, args):
    """
    Serializes and saves a trained model pipeline along with threshold and metadata.

    This function creates a `.pkl` file containing:
        - The trained model object (e.g., a scikit-learn pipeline)
        - A decision threshold (e.g., for classification probability cutoffs)
        - A human-readable model name

    The filename is automatically generated by lowercasing the model name,
    replacing spaces with underscores, and appending `.pkl`. The file is saved
    in the directory specified by `args.save_dir`. The directory is created
    if it does not exist.

    Parameters:
    -----------
    model : object
        A trained model or pipeline compatible with scikit-learn (must support `.predict`).
    
    threshold : float
        Classification threshold to be saved with the model (useful for post-processing).
    
    args : argparse.Namespace
        A namespace object with the following required attributes:
            - model_name (str): Name of the model (used in metadata and filename).
            - save_dir (str or Path): Directory path where the model file will be saved.

    Returns:
    --------
    None
        Saves the model to disk and prints a confirmation message.
    
    Example:
    --------
    args.model_name = "Logistic Regression"
    args.save_dir = "./models"
    
    → File will be saved as: ./models/logistic_regression.pkl
    """
    # Convert model name to lowercase and replace spaces for filename
    filename = Path(args.save_dir) / f"{args.model_name}.pkl"

    # Create directory if it doesn't exist
    Path(args.save_dir).mkdir(parents=True, exist_ok=True)

    model_bundle = {
        'pipeline': pipeline,
        'threshold': threshold,
        'model_name': args.model_name
    }

    with open(filename, 'wb') as file:
        pickle.dump(model_bundle, file)

    print(f"\nModel saved as '{filename}'")


if __name__ == '__main__':
    args = parse_args()

    print('\nModel: {}, Data scaling: {}, Balance strategy: {}, GridSearch:{}, PCA:{}'.format(args.model, args.scaling, args.balance, args.grid_search, args.use_pca))
    print()

    X_train, X_test, y_train, y_test = load_data(args.train_dir, args.test_dir)

    scaler = get_scaler(args.scaling)

    balance_strategy = get_balance_strategy(args)
    pipeline = build_model(args, scaler, balance_strategy)

    
    if args.grid_search:
        start = time.time()

        cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
        grid = setup_grid(pipeline, args, cv= cv, scoring='f1')
        
        grid.fit(X_train, y_train)
        pipeline = grid.best_estimator_

        print(f"GridSearch Time is {(time.time() - start)/60 :.3f} mintues")
        print(f'{args.model} best parameters is:', grid.best_params_)
        print("Best cross-validation F1 score:", grid.best_score_)

    else:
        pipeline.fit(X_train, y_train)

    threshold = find_best_threshold(pipeline, X_test, y_test)

    evaluate_with_threshold(pipeline, X_train, y_train, 'Training', threshold)
    print()
    evaluate_with_threshold(pipeline, X_test, y_test, 'Testing', threshold)

    save_model_bundle(pipeline, threshold, args)
